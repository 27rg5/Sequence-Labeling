{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a921c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272bc06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f726ebf230>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FOR REPRODUCIBILTY\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False \n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b74417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING GPU\n",
    "# print(torch.cuda.device_count())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8a0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open(\"data/train\", \"r\")\n",
    "lines = train.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78523f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for i in range(len(lines)):\n",
    "    if lines[i] == '\\n':\n",
    "        continue\n",
    "    \n",
    "    line = lines[i].split(' ')\n",
    "    if line[1] not in vocab:\n",
    "        vocab[line[1]] = 1\n",
    "    else:\n",
    "        vocab[line[1]] = 1 + vocab[line[1]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e330014",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1\n",
    "unk_dict = {'< unk >': 0}\n",
    "   \n",
    "for word in vocab:\n",
    "    if vocab[word] <= threshold:\n",
    "        unk_dict['< unk >'] = vocab[word] + unk_dict['< unk >']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "697720e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab = vocab.copy()\n",
    "for word in vocab:\n",
    "    if final_vocab[word] <= threshold:\n",
    "        del final_vocab[word]\n",
    "final_vocab['< unk >'] = unk_dict['< unk >']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fa1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "word_index = {}\n",
    "word_index['pad'] = 0\n",
    "for word in final_vocab:\n",
    "    if word not in word_index:\n",
    "        word_index[word] = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a134adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "tag_index = {}\n",
    "for line in lines:\n",
    "    if line == '\\n':\n",
    "        continue\n",
    "    word = line.split(' ')[2].strip()\n",
    "    if word not in tag_index:\n",
    "        tag_index[word] = len(tag_index)\n",
    "\n",
    "tag_index['pad'] = -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2982137",
   "metadata": {},
   "source": [
    "# Bi-LSTM with custom embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36dde1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(data):\n",
    "    sent_tag_pair = []\n",
    "    tx, ty = [], []\n",
    "    for line in data:\n",
    "        if line == '\\n':\n",
    "            sent_tag_pair.append([tx, ty])\n",
    "            tx,ty = [], []\n",
    "            continue\n",
    "            \n",
    "        if line.split(' ')[1] not in word_index:\n",
    "            tx.append(word_index['< unk >'])\n",
    "        else:\n",
    "            tx.append(word_index[line.split(' ')[1]])\n",
    "        ty.append(tag_index[line.split(' ')[2].strip()])\n",
    "    sent_tag_pair.append([tx, ty]) \n",
    "    return sent_tag_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "229bb606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):    \n",
    "    x, y, x_len = [], [], []\n",
    "    \n",
    "    for sent, tag in batch:\n",
    "        x_len.append(len(sent))\n",
    "        x.append(torch.LongTensor(sent))\n",
    "        y.append(torch.LongTensor(tag))\n",
    "    padding1 = torch.nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=0)\n",
    "    padding2 = torch.nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    return padding1, padding2, x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fe75123",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_pairs = create_pairs(lines)\n",
    "train_dataloader = DataLoader(train_pairs, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=collate_fn, worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5260d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = open(\"data/dev\", \"r\")\n",
    "lines = val.readlines()\n",
    "val_pair = create_pairs(lines)\n",
    "val_dataloader = DataLoader(val_pair, batch_size=batch_size, shuffle=False, pin_memory=True, collate_fn=collate_fn, worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70510ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embed = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a4eecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings = num_embed, embedding_dim = 100)\n",
    "        self.bilstm = nn.LSTM(input_size=100, hidden_size=256, num_layers=1, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear = nn.Linear(512, 128)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier1 = nn.Linear(128, 9)\n",
    "        \n",
    "    def forward(self, x, x_len):\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
    "        x, (hn, cn) = self.bilstm(x)\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.classifier1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86081364",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = BiLSTM1().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c89cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optim=torch.optim.SGD(model1.parameters(), lr= 0.25, momentum = 0.9)\n",
    "loss = nn.CrossEntropyLoss(ignore_index=-1).to(device)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optim, mode='max', patience=3, min_lr=0.05, factor = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79dfc9a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n",
      "Epoch: 1\n",
      "Train Loss: 0.07867799774746598 \tTrain F1: 0.23591345899195806\n",
      "Val Loss: 0.06545669195555529 \tVal F1: 0.33104536176854715\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 2\n",
      "Train Loss: 0.05778657596284005 \tTrain F1: 0.4048153410186125\n",
      "Val Loss: 0.04726696045412618 \tVal F1: 0.49378618162752996\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 3\n",
      "Train Loss: 0.04772625027449052 \tTrain F1: 0.5071964385392648\n",
      "Val Loss: 0.03843432343253771 \tVal F1: 0.5912726500275806\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 4\n",
      "Train Loss: 0.04105193667057126 \tTrain F1: 0.5666569718309127\n",
      "Val Loss: 0.03363436125116938 \tVal F1: 0.6321070707571649\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 5\n",
      "Train Loss: 0.036416480243911725 \tTrain F1: 0.6129901693733626\n",
      "Val Loss: 0.030330017866906905 \tVal F1: 0.672246205902111\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 6\n",
      "Train Loss: 0.033209020711964625 \tTrain F1: 0.6463920040702174\n",
      "Val Loss: 0.027909679907470113 \tVal F1: 0.689330620942138\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 7\n",
      "Train Loss: 0.030697176721455926 \tTrain F1: 0.6704772558914882\n",
      "Val Loss: 0.02564181013530778 \tVal F1: 0.7079943956177993\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 8\n",
      "Train Loss: 0.028631702705317254 \tTrain F1: 0.6909000195740018\n",
      "Val Loss: 0.02458720622623923 \tVal F1: 0.7209986091693324\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 9\n",
      "Train Loss: 0.027244728228416226 \tTrain F1: 0.7025784375383797\n",
      "Val Loss: 0.024351616076546213 \tVal F1: 0.7308603604629547\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 10\n",
      "Train Loss: 0.025794971871573435 \tTrain F1: 0.7187032921649003\n",
      "Val Loss: 0.02328900166759109 \tVal F1: 0.7363309614330583\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 11\n",
      "Train Loss: 0.024229874442743483 \tTrain F1: 0.7298032752959385\n",
      "Val Loss: 0.02362774681464355 \tVal F1: 0.7418265569989769\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 12\n",
      "Train Loss: 0.023495523991081128 \tTrain F1: 0.7399168230104765\n",
      "Val Loss: 0.02191634270008683 \tVal F1: 0.7484275003438542\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 13\n",
      "Train Loss: 0.022517722717877935 \tTrain F1: 0.746749336915617\n",
      "Val Loss: 0.021543875270576645 \tVal F1: 0.7583549790284584\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 14\n",
      "Train Loss: 0.021399798070745525 \tTrain F1: 0.7623037606804535\n",
      "Val Loss: 0.020855369742955196 \tVal F1: 0.75820107461231\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 15\n",
      "Train Loss: 0.020655030658366227 \tTrain F1: 0.767534720273339\n",
      "Val Loss: 0.020644073086552982 \tVal F1: 0.7631920743183738\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 16\n",
      "Train Loss: 0.020221875931482697 \tTrain F1: 0.7757082663486574\n",
      "Val Loss: 0.020941949151148553 \tVal F1: 0.7662798116051774\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 17\n",
      "Train Loss: 0.019457590385820227 \tTrain F1: 0.7814691499451484\n",
      "Val Loss: 0.020015640416137345 \tVal F1: 0.7749332138846449\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 18\n",
      "Train Loss: 0.01897480869240832 \tTrain F1: 0.7851677549722852\n",
      "Val Loss: 0.02052108581920834 \tVal F1: 0.7727948967703014\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 19\n",
      "Train Loss: 0.018395543629959574 \tTrain F1: 0.7894499917199251\n",
      "Val Loss: 0.020215318836669677 \tVal F1: 0.7745746800157143\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 20\n",
      "Train Loss: 0.017790604401327643 \tTrain F1: 0.7967099883758854\n",
      "Val Loss: 0.020339496450629702 \tVal F1: 0.7795599941583461\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 21\n",
      "Train Loss: 0.017219704317380978 \tTrain F1: 0.8038491860886996\n",
      "Val Loss: 0.019534552534965426 \tVal F1: 0.7842377122794417\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 22\n",
      "Train Loss: 0.0169526895535752 \tTrain F1: 0.8046410475566361\n",
      "Val Loss: 0.01959694755596334 \tVal F1: 0.7848977163452999\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 23\n",
      "Train Loss: 0.01658989556166423 \tTrain F1: 0.8061585472347047\n",
      "Val Loss: 0.019391052934986473 \tVal F1: 0.7848774484552862\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 24\n",
      "Train Loss: 0.016066787156479915 \tTrain F1: 0.8140358364853747\n",
      "Val Loss: 0.01958019564240874 \tVal F1: 0.7820030864524317\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 25\n",
      "Train Loss: 0.015563206400754816 \tTrain F1: 0.8147743449302005\n",
      "Val Loss: 0.020301027105698217 \tVal F1: 0.7807846025484927\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 26\n",
      "Train Loss: 0.015322889726827162 \tTrain F1: 0.8243468810183269\n",
      "Val Loss: 0.01948824155616591 \tVal F1: 0.7789304663778348\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 27\n",
      "Train Loss: 0.014480534156925058 \tTrain F1: 0.8296632663194526\n",
      "Val Loss: 0.019015068247722307 \tVal F1: 0.7888080629983716\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 28\n",
      "Train Loss: 0.013809524744876666 \tTrain F1: 0.8404673978867889\n",
      "Val Loss: 0.01913527244493245 \tVal F1: 0.7954441514966978\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Epoch: 29\n",
      "Train Loss: 0.013448321296402457 \tTrain F1: 0.845557267642567\n",
      "Val Loss: 0.01912531366960466 \tVal F1: 0.7906581398707588\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Epoch: 30\n",
      "Train Loss: 0.013359407900354631 \tTrain F1: 0.843675582882798\n",
      "Val Loss: 0.019101908059088582 \tVal F1: 0.7900447474885457\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Epoch: 31\n",
      "Train Loss: 0.012883061662830314 \tTrain F1: 0.8480676079237286\n",
      "Val Loss: 0.01974154486222264 \tVal F1: 0.7951277160431568\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 32\n",
      "Train Loss: 0.013040274403675105 \tTrain F1: 0.8470396902959676\n",
      "Val Loss: 0.019443027122015066 \tVal F1: 0.7958926063157489\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 33\n",
      "Train Loss: 0.012556598176219298 \tTrain F1: 0.8562726522250376\n",
      "Val Loss: 0.01927687848824132 \tVal F1: 0.7979666985791325\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34\n",
      "Train Loss: 0.012350828680009163 \tTrain F1: 0.8518716769188465\n",
      "Val Loss: 0.019483582818598473 \tVal F1: 0.793471594778027\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Epoch: 35\n",
      "Train Loss: 0.012201542736125788 \tTrain F1: 0.8537382057180424\n",
      "Val Loss: 0.019251469408431577 \tVal F1: 0.7957682153399699\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Epoch: 36\n",
      "Train Loss: 0.012255942605239793 \tTrain F1: 0.8543555477874506\n",
      "Val Loss: 0.019256249102582175 \tVal F1: 0.7974217758967491\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Epoch: 37\n",
      "Train Loss: 0.011706962330573589 \tTrain F1: 0.8612756059064756\n",
      "Val Loss: 0.019230204349384595 \tVal F1: 0.7955498542667698\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 38\n",
      "Train Loss: 0.011689812270011266 \tTrain F1: 0.8598685525939438\n",
      "Val Loss: 0.018976059118817997 \tVal F1: 0.7993409513360921\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 39\n",
      "Train Loss: 0.011355801283452635 \tTrain F1: 0.866671281526383\n",
      "Val Loss: 0.019295098170367356 \tVal F1: 0.8018556544287061\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Epoch: 40\n",
      "Train Loss: 0.011337608877025908 \tTrain F1: 0.8679826103166366\n",
      "Val Loss: 0.019267729777570422 \tVal F1: 0.7954461232195156\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Epoch: 41\n",
      "Train Loss: 0.01114413558757129 \tTrain F1: 0.8683753387094252\n",
      "Val Loss: 0.019341815839720663 \tVal F1: 0.7980432829378337\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Epoch: 42\n",
      "Train Loss: 0.010980438230384793 \tTrain F1: 0.8679656529262368\n",
      "Val Loss: 0.019356760594067993 \tVal F1: 0.7981861733613584\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Epoch: 43\n",
      "Train Loss: 0.01074834214256313 \tTrain F1: 0.8739477070600784\n",
      "Val Loss: 0.019529534265820714 \tVal F1: 0.7926934777602443\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 44\n",
      "Train Loss: 0.010821314533041684 \tTrain F1: 0.8719491963382869\n",
      "Val Loss: 0.019500218840206942 \tVal F1: 0.7984588169191277\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 45\n",
      "Train Loss: 0.010821281103037095 \tTrain F1: 0.8703585535528734\n",
      "Val Loss: 0.019274454331419978 \tVal F1: 0.7980844872213719\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 46\n",
      "Train Loss: 0.010477742319985635 \tTrain F1: 0.8755646854328628\n",
      "Val Loss: 0.019204075618747624 \tVal F1: 0.7985942897220186\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 47\n",
      "Train Loss: 0.010550238692942223 \tTrain F1: 0.8747063928052835\n",
      "Val Loss: 0.019336608642487128 \tVal F1: 0.7987833389283046\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 48\n",
      "Train Loss: 0.010666426939804 \tTrain F1: 0.8728392633672707\n",
      "Val Loss: 0.019256641300276857 \tVal F1: 0.7998967160777798\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 49\n",
      "Train Loss: 0.010382958710127909 \tTrain F1: 0.87839179759441\n",
      "Val Loss: 0.019133533208921007 \tVal F1: 0.7967214205288796\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 50\n",
      "Train Loss: 0.010309547935717485 \tTrain F1: 0.8743899148645216\n",
      "Val Loss: 0.019619092797156273 \tVal F1: 0.7959001928308823\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Final Val accuracy: 0.7959001928308823\n"
     ]
    }
   ],
   "source": [
    "history_train = []\n",
    "history_val = []\n",
    "train_dataloader_len = len(train_dataloader)\n",
    "val_dataloader_len = len(val_dataloader)\n",
    "highest_val_score = -np.inf\n",
    "for epoch in range(50):  \n",
    "    model1.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    train_accuracy = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        final_label = []\n",
    "        final_output = []\n",
    "        inputs, label, inputs_len = data\n",
    "        inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "        model_optim.zero_grad()\n",
    "        output = model1(inputs, inputs_len)\n",
    "        output = output.to(device)\n",
    "\n",
    "        output = output.view(-1, output.size()[-1])\n",
    "        label = label.view(-1)\n",
    "\n",
    "        loss1 = loss(output, label)/batch_size\n",
    "        loss1.backward()\n",
    "        model_optim.step()\n",
    "\n",
    "        train_loss += loss1.item()\n",
    "\n",
    "        _, output = torch.max(output, 1)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        label = label.cpu().detach().numpy()\n",
    "        for ii in range(len(label)):\n",
    "            if label[ii] != -1:\n",
    "                final_label.append(label[ii])\n",
    "                final_output.append(output[ii])\n",
    "        train_accuracy += f1_score(final_label, final_output, average='macro')\n",
    "    \n",
    "    train_loss = train_loss/train_dataloader_len\n",
    "    train_accuracy = train_accuracy/train_dataloader_len\n",
    "    history_train.append((train_loss, train_accuracy))\n",
    "    \n",
    "    model1.eval()   \n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    correct_val = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            final_label = []\n",
    "            final_output = []\n",
    "            inputs, label, inputs_len = data\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "            pred = model1(inputs, inputs_len)\n",
    "            pred = pred.to(device)\n",
    "            \n",
    "            pred = pred.view(-1, pred.size()[-1])\n",
    "            label = label.view(-1)\n",
    "            \n",
    "            loss2 = loss(pred, label)/batch_size\n",
    "            val_loss += loss2.item()\n",
    "            _, pred = torch.max(pred, 1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            label = label.cpu().detach().numpy()\n",
    "            for ii in range(len(label)):\n",
    "                if label[ii] != -1:\n",
    "                    final_label.append(label[ii])\n",
    "                    final_output.append(pred[ii])\n",
    "            \n",
    "            val_accuracy += f1_score(final_label, final_output, average='macro')\n",
    "            \n",
    "        val_loss = val_loss/val_dataloader_len\n",
    "        val_accuracy = val_accuracy/val_dataloader_len\n",
    "        scheduler.step(val_accuracy)\n",
    "        history_val.append((val_loss, val_accuracy))\n",
    "        if val_accuracy > highest_val_score:\n",
    "            print(\"Saving model\")\n",
    "            highest_val_score = val_accuracy\n",
    "            torch.save({'model_state_dict': model1.state_dict()}, 'blstm1.pt')\n",
    "            \n",
    "    print(\"Epoch:\", (epoch+1))\n",
    "    print(\"Train Loss:\", train_loss, \"\\tTrain F1:\", train_accuracy)\n",
    "    print(\"Val Loss:\", val_loss, \"\\tVal F1:\", val_accuracy)\n",
    "    print(\"Current learning rate:\", model_optim.param_groups[0]['lr'])\n",
    "    print(\"=======================================================================================\")\n",
    "\n",
    "print(\"Final Val accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3acb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []\n",
    "final_label = []\n",
    "model1.load_state_dict(torch.load('blstm1.pt'), strict=False)\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(val_dataloader, 0):\n",
    "\n",
    "        inputs, label, inputs_len = data\n",
    "        inputs, label = inputs.to(device), label.to(device)\n",
    "        pred = model1(inputs, inputs_len)\n",
    "        pred = pred.to(device)\n",
    "\n",
    "        pred = pred.view(-1, pred.size()[-1])\n",
    "        label = label.view(-1)\n",
    "        label = label.cpu().detach().numpy()\n",
    "\n",
    "        _, pred = torch.max(pred, 1)\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        \n",
    "        for ii in range(len(label)):\n",
    "            if label[ii] != -1:\n",
    "                final_output.append(pred[ii])\n",
    "                final_label.append(label[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dabbabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = open('data/dev', 'r')\n",
    "lines = val.readlines()\n",
    "\n",
    "all_sentences = []\n",
    "cnt = 0\n",
    "for line in lines:\n",
    "    if line == '\\n':\n",
    "         all_sentences.append(line)\n",
    "    else:\n",
    "        cnt += 1\n",
    "        all_sentences.append(line.split(' '))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b243a5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "val_key = {val:key for key, val in tag_index.items()}\n",
    "with open(\"eval_dev1.out\", \"w\") as f:\n",
    "    for splits in all_sentences:\n",
    "            if splits == \"\\n\":\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "\n",
    "                idx, word, tag = splits\n",
    "                f.write(f\"{idx} {word} {tag.strip()} {val_key[final_output[i]]}\\n\")\n",
    "                i += 1\n",
    "i = 0         \n",
    "with open(\"dev1.out\", \"w\") as f:\n",
    "    for splits in all_sentences:\n",
    "            if splits == \"\\n\":\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "\n",
    "                idx, word, tag = splits\n",
    "                f.write(f\"{idx} {word} {val_key[final_output[i]]}\\n\")\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2f818d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs_test(data):\n",
    "    sent_tag_pair = []\n",
    "    tx = []\n",
    "    for line in data:\n",
    "        if line == '\\n':\n",
    "            sent_tag_pair.append(tx)\n",
    "            tx = []\n",
    "            continue\n",
    "            \n",
    "        if line.split(' ')[1] not in word_index:\n",
    "            tx.append(word_index['< unk >'])\n",
    "        else:\n",
    "            tx.append(word_index[line.split(' ')[1]])\n",
    "    sent_tag_pair.append(tx) \n",
    "    return sent_tag_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c324e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_test(batch):    \n",
    "    x, x_len = [], []\n",
    "    \n",
    "    for sent in batch:\n",
    "        x_len.append(len(sent))\n",
    "        x.append(torch.LongTensor(sent))\n",
    "    padding1 = torch.nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padding1, x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e2dbc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open(\"data/test\", \"r\")\n",
    "lines = test.readlines()\n",
    "test_pair = create_pairs_test(lines)\n",
    "test_dataloader = DataLoader(test_pair, batch_size=8, shuffle=False, pin_memory=True, collate_fn=collate_fn_test, worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29450435",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []\n",
    "final_label = []\n",
    "model1.load_state_dict(torch.load('blstm1.pt'), strict=False)\n",
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader, 0):\n",
    "\n",
    "        inputs, inputs_len = data\n",
    "        inputs = inputs.to(device)\n",
    "        pred = model1(inputs, inputs_len)\n",
    "        pred = pred.to(device)\n",
    "\n",
    "        pred = pred.view(-1, pred.size()[-1])\n",
    "\n",
    "        _, pred = torch.max(pred, 1)\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        \n",
    "        for ii in range(len(pred)):\n",
    "            if pred[ii] != -1:\n",
    "                final_output.append(pred[ii])\n",
    "\n",
    "test = open('data/test', 'r')\n",
    "lines = test.readlines()\n",
    "\n",
    "all_sentences = []\n",
    "cnt = 0\n",
    "for line in lines:\n",
    "    if line == '\\n':\n",
    "         all_sentences.append(line)\n",
    "    else:\n",
    "        cnt += 1\n",
    "        all_sentences.append(line.split(' '))\n",
    "        \n",
    "i = 0         \n",
    "test_key = {test:key for key, test in tag_index.items()}\n",
    "with open(\"test1.out\", \"w\") as f:\n",
    "    for splits in all_sentences:\n",
    "            if splits == \"\\n\":\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "\n",
    "                idx, word = splits\n",
    "                f.write(f\"{idx} {word.strip()} {test_key[final_output[i]]}\\n\")\n",
    "                i += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a6deaea",
   "metadata": {},
   "source": [
    "# BiLSTM with GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1c89773",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embedding = np.array([float(val) for val in values[1:]])\n",
    "        glove_embeddings[word] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "325ea03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = np.zeros((len(word_index), 100))\n",
    "# for word, index in word_index.items():\n",
    "#     if word.lower() in glove_embeddings:\n",
    "#         embeddings[index] = glove_embeddings[word.lower()]\n",
    "#     elif word in glove_embeddings:\n",
    "#         embeddings[index] = glove_embeddings[word]\n",
    "#     else:\n",
    "#         embeddings[index] = np.random.uniform(low=-0.25, high=0.25, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b03527fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(word_index), 101))\n",
    "temp_num=np.sqrt(3.0/100.0)\n",
    "temp = np.zeros((len(word_index), 100))\n",
    "for word, index in word_index.items():\n",
    "    if (word.lower() == word) and (word in glove_embeddings):\n",
    "        temp[index] = glove_embeddings[word.lower()]\n",
    "        embeddings[index] = np.append(temp[index], 0)\n",
    "    elif (word.lower() == word) and (word not in glove_embeddings):\n",
    "        temp[index] = np.random.uniform(low=-temp_num, high=temp_num, size=100)\n",
    "        embeddings[index] = np.append(temp[index], 2)\n",
    "    elif (word.lower() != word) and (word in glove_embeddings):\n",
    "        temp[index] = glove_embeddings[word]\n",
    "        embeddings[index] = np.append(temp[index], 1)\n",
    "    else:\n",
    "        temp[index] = np.random.uniform(low=-temp_num, high=temp_num, size=100)\n",
    "        embeddings[index] = np.append(temp[index], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b19ba13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(len(word_index), 101)\n",
    "embedding_layer.load_state_dict({\"weight\":torch.FloatTensor(embeddings)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2dd7988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_linear_layer(layer):\n",
    "    nn.init.xavier_normal_(layer.weight.data)\n",
    "    nn.init.normal_(layer.bias.data)\n",
    "\n",
    "def initialize_lstm_layer(layer):\n",
    "    for param in layer.parameters():\n",
    "        if len(param.shape) >= 2:\n",
    "            nn.init.orthogonal_(param.data)\n",
    "        else:\n",
    "            nn.init.normal_(param.data)\n",
    "\n",
    "class BiLSTM2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = embedding_layer\n",
    "        self.bilstm = nn.LSTM(input_size=101, hidden_size=256, num_layers=1, bidirectional=True)\n",
    "        initialize_lstm_layer(self.bilstm)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear = nn.Linear(512, 128)\n",
    "        initialize_linear_layer(self.linear)\n",
    "        self.elu = nn.ELU()\n",
    "        self.classifier1 = nn.Linear(128, 9)\n",
    "        initialize_linear_layer(self.classifier1)\n",
    "    \n",
    "        \n",
    "    def forward(self, x, x_len):\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
    "        x, (hn, cn) = self.bilstm(x)\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.elu(x)\n",
    "#         x = self.classifier1(x)\n",
    "        x = self.classifier1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b66c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BiLSTM2().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50b83031",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optim=torch.optim.SGD(model2.parameters(), lr= 0.25, momentum = 0.95)\n",
    "loss = nn.CrossEntropyLoss(ignore_index=-1).to(device)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optim, mode='max', patience=2, min_lr=0.05, factor = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a07533c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n",
      "Epoch: 1\n",
      "Train Loss: 0.0471497024896504 \tTrain F1: 0.39625359162286616\n",
      "Val Loss: 0.03453427309013178 \tVal F1: 0.5490869725306171\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 2\n",
      "Train Loss: 0.02731319518015385 \tTrain F1: 0.6160794072332325\n",
      "Val Loss: 0.02502042608726437 \tVal F1: 0.6628649676814182\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 3\n",
      "Train Loss: 0.01928349273287261 \tTrain F1: 0.7356017806285772\n",
      "Val Loss: 0.024047875464791948 \tVal F1: 0.7002817586077482\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 4\n",
      "Train Loss: 0.014979772003473895 \tTrain F1: 0.8004019640633508\n",
      "Val Loss: 0.01880057969783635 \tVal F1: 0.7493376617010018\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 5\n",
      "Train Loss: 0.012269328069052874 \tTrain F1: 0.8358121978634601\n",
      "Val Loss: 0.01845233304767692 \tVal F1: 0.791353176618177\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 6\n",
      "Train Loss: 0.010618202063298995 \tTrain F1: 0.8654851635330121\n",
      "Val Loss: 0.01880726633348807 \tVal F1: 0.7844529786749549\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 7\n",
      "Train Loss: 0.009072186623339909 \tTrain F1: 0.8821008410798381\n",
      "Val Loss: 0.015878044848225137 \tVal F1: 0.8013449063832307\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 8\n",
      "Train Loss: 0.008332313934733741 \tTrain F1: 0.8949370496178121\n",
      "Val Loss: 0.015299187105159952 \tVal F1: 0.8015509860530243\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 9\n",
      "Train Loss: 0.007421222970582542 \tTrain F1: 0.9078976593046012\n",
      "Val Loss: 0.014413250541938523 \tVal F1: 0.8233118660663464\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 10\n",
      "Train Loss: 0.006963464837843794 \tTrain F1: 0.9148433760339497\n",
      "Val Loss: 0.01493117003933297 \tVal F1: 0.8119142415876851\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 11\n",
      "Train Loss: 0.006647209835389259 \tTrain F1: 0.9170728525448115\n",
      "Val Loss: 0.015477356011512799 \tVal F1: 0.8007577984271016\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 12\n",
      "Train Loss: 0.006184317499527528 \tTrain F1: 0.9218039159304868\n",
      "Val Loss: 0.01659319621124175 \tVal F1: 0.8263754561253392\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 13\n",
      "Train Loss: 0.005600625387220288 \tTrain F1: 0.9282451524592218\n",
      "Val Loss: 0.016141465477810937 \tVal F1: 0.8032485137194808\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 14\n",
      "Train Loss: 0.005358904071625578 \tTrain F1: 0.9325565902322002\n",
      "Val Loss: 0.015081883192284276 \tVal F1: 0.8379757710222612\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 15\n",
      "Train Loss: 0.00502924392067153 \tTrain F1: 0.9366389975209876\n",
      "Val Loss: 0.014509338495232622 \tVal F1: 0.8265497864716829\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 16\n",
      "Train Loss: 0.0049724476157830995 \tTrain F1: 0.9343767899978174\n",
      "Val Loss: 0.014779750416794218 \tVal F1: 0.8290651492033074\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 17\n",
      "Train Loss: 0.004580819770907138 \tTrain F1: 0.939641735560718\n",
      "Val Loss: 0.014554840811172722 \tVal F1: 0.8438736384175333\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 18\n",
      "Train Loss: 0.004351159013404948 \tTrain F1: 0.9435222678697651\n",
      "Val Loss: 0.014609921005134299 \tVal F1: 0.8397459831063582\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 19\n",
      "Train Loss: 0.00420716981466764 \tTrain F1: 0.9465861485493926\n",
      "Val Loss: 0.01563099919050765 \tVal F1: 0.8335905621945758\n",
      "Current learning rate: 0.25\n",
      "=======================================================================================\n",
      "Epoch: 20\n",
      "Train Loss: 0.003958086449389082 \tTrain F1: 0.9484934448286938\n",
      "Val Loss: 0.016079513975914915 \tVal F1: 0.8356786804192642\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Epoch: 21\n",
      "Train Loss: 0.003292549785094036 \tTrain F1: 0.9575038166860519\n",
      "Val Loss: 0.015200939135681848 \tVal F1: 0.8406744607838242\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Epoch: 22\n",
      "Train Loss: 0.0032176576866133517 \tTrain F1: 0.9573820272931121\n",
      "Val Loss: 0.015209112438243872 \tVal F1: 0.8390859675090719\n",
      "Current learning rate: 0.125\n",
      "=======================================================================================\n",
      "Epoch: 23\n",
      "Train Loss: 0.0030522338565774887 \tTrain F1: 0.9578089355248782\n",
      "Val Loss: 0.015180001465012706 \tVal F1: 0.8428940780364058\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Epoch: 24\n",
      "Train Loss: 0.002805769606860172 \tTrain F1: 0.9629491708768447\n",
      "Val Loss: 0.015063279472098024 \tVal F1: 0.8390139885338671\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 25\n",
      "Train Loss: 0.0027333218216229476 \tTrain F1: 0.9664113849983603\n",
      "Val Loss: 0.015722397832386582 \tVal F1: 0.8466150411644484\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Epoch: 26\n",
      "Train Loss: 0.002697331173034852 \tTrain F1: 0.966599317857829\n",
      "Val Loss: 0.015493225606238567 \tVal F1: 0.8387545061231932\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Epoch: 27\n",
      "Train Loss: 0.0026305444592904055 \tTrain F1: 0.9650949524201263\n",
      "Val Loss: 0.015695103227025126 \tVal F1: 0.8460141104282773\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 28\n",
      "Train Loss: 0.0026874936286590597 \tTrain F1: 0.9646001215436156\n",
      "Val Loss: 0.015862150480128064 \tVal F1: 0.8472312265889419\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Epoch: 29\n",
      "Train Loss: 0.0025984669269082717 \tTrain F1: 0.9660966602168571\n",
      "Val Loss: 0.015843071681973443 \tVal F1: 0.8421045361846117\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Epoch: 30\n",
      "Train Loss: 0.002616760178515083 \tTrain F1: 0.9652653749473745\n",
      "Val Loss: 0.015958227497156873 \tVal F1: 0.8451260122021567\n",
      "Current learning rate: 0.0625\n",
      "=======================================================================================\n",
      "Epoch: 31\n",
      "Train Loss: 0.002551802949158788 \tTrain F1: 0.9648784392909956\n",
      "Val Loss: 0.016048387617647212 \tVal F1: 0.8458042622160347\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 32\n",
      "Train Loss: 0.0023745139307327465 \tTrain F1: 0.9687883687819248\n",
      "Val Loss: 0.016249072564987505 \tVal F1: 0.8490445394347954\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 33\n",
      "Train Loss: 0.002376839030038235 \tTrain F1: 0.9690784324789423\n",
      "Val Loss: 0.01566348125790468 \tVal F1: 0.8468602643330829\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34\n",
      "Train Loss: 0.002425316671603574 \tTrain F1: 0.966929665427905\n",
      "Val Loss: 0.015834246621577282 \tVal F1: 0.8476351232560113\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 35\n",
      "Train Loss: 0.002379694611325289 \tTrain F1: 0.9697128128118093\n",
      "Val Loss: 0.016093871547112594 \tVal F1: 0.8465794899477006\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 36\n",
      "Train Loss: 0.0023959812382354346 \tTrain F1: 0.9699514691130172\n",
      "Val Loss: 0.01608705014845386 \tVal F1: 0.8478485074418692\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 37\n",
      "Train Loss: 0.0023644579484196826 \tTrain F1: 0.9692606205657636\n",
      "Val Loss: 0.015794912256052836 \tVal F1: 0.8476792285573318\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 38\n",
      "Train Loss: 0.0023388570395715628 \tTrain F1: 0.9694453844420423\n",
      "Val Loss: 0.016168860858158815 \tVal F1: 0.8463237665907798\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Saving model\n",
      "Epoch: 39\n",
      "Train Loss: 0.0022933447988557863 \tTrain F1: 0.9709804811613686\n",
      "Val Loss: 0.016210232250147727 \tVal F1: 0.8540286842349515\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 40\n",
      "Train Loss: 0.002300628743734369 \tTrain F1: 0.969305588105645\n",
      "Val Loss: 0.016088611501133384 \tVal F1: 0.8412442238457051\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 41\n",
      "Train Loss: 0.0022483323313383934 \tTrain F1: 0.9721343589568967\n",
      "Val Loss: 0.016474663169860133 \tVal F1: 0.8442589003058673\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 42\n",
      "Train Loss: 0.0021775464213782845 \tTrain F1: 0.9724501407238596\n",
      "Val Loss: 0.01652777474115535 \tVal F1: 0.8480560318462608\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 43\n",
      "Train Loss: 0.0021715323660317365 \tTrain F1: 0.972592419967185\n",
      "Val Loss: 0.01704197891554719 \tVal F1: 0.8436363335561698\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 44\n",
      "Train Loss: 0.002184029602173995 \tTrain F1: 0.9700090463585688\n",
      "Val Loss: 0.01639515181108379 \tVal F1: 0.8424219777308259\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 45\n",
      "Train Loss: 0.0021772171851283666 \tTrain F1: 0.9716391640113767\n",
      "Val Loss: 0.0163997608837957 \tVal F1: 0.8524572678002829\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 46\n",
      "Train Loss: 0.0021839572636257725 \tTrain F1: 0.9714052772354365\n",
      "Val Loss: 0.016575384705174223 \tVal F1: 0.8510560814582244\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 47\n",
      "Train Loss: 0.0020693980106685442 \tTrain F1: 0.9714687931338554\n",
      "Val Loss: 0.01656735482775208 \tVal F1: 0.849790659112458\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 48\n",
      "Train Loss: 0.0021080922663409074 \tTrain F1: 0.9717803415407112\n",
      "Val Loss: 0.016700436974517265 \tVal F1: 0.8454772687160086\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 49\n",
      "Train Loss: 0.002058866302293989 \tTrain F1: 0.9721553843037876\n",
      "Val Loss: 0.017309535532888748 \tVal F1: 0.8476594701698059\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 50\n",
      "Train Loss: 0.0020844011987002503 \tTrain F1: 0.9710638385238229\n",
      "Val Loss: 0.016888669502108065 \tVal F1: 0.8463583345096961\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 51\n",
      "Train Loss: 0.002050284553532492 \tTrain F1: 0.9726763772799453\n",
      "Val Loss: 0.01719399003889933 \tVal F1: 0.8458577403217972\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 52\n",
      "Train Loss: 0.002024241138981346 \tTrain F1: 0.9725422518285372\n",
      "Val Loss: 0.01694634191315354 \tVal F1: 0.8451891710288046\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 53\n",
      "Train Loss: 0.0020026512510449256 \tTrain F1: 0.9749471851049002\n",
      "Val Loss: 0.017026926252670018 \tVal F1: 0.8414567888146122\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 54\n",
      "Train Loss: 0.0019687100793591406 \tTrain F1: 0.9729252658105997\n",
      "Val Loss: 0.01720437100289312 \tVal F1: 0.837544971176691\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Epoch: 55\n",
      "Train Loss: 0.0019303061503962364 \tTrain F1: 0.9755500211163133\n",
      "Val Loss: 0.017202272982478947 \tVal F1: 0.8453267994024779\n",
      "Current learning rate: 0.05\n",
      "=======================================================================================\n",
      "Final Val accuracy: 0.8453267994024779\n"
     ]
    }
   ],
   "source": [
    "history_train = []\n",
    "history_val = []\n",
    "train_dataloader_len = len(train_dataloader)\n",
    "val_dataloader_len = len(val_dataloader)\n",
    "highest_val_score = -np.inf\n",
    "for epoch in range(55):  \n",
    "    model2.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    train_accuracy = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        final_label = []\n",
    "        final_output = []\n",
    "        inputs, label, inputs_len = data\n",
    "        inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "        model_optim.zero_grad()\n",
    "        output = model2(inputs, inputs_len)\n",
    "        output = output.to(device)\n",
    "\n",
    "        output = output.view(-1, output.size()[-1])\n",
    "        label = label.view(-1)\n",
    "\n",
    "        loss1 = loss(output, label)/batch_size\n",
    "        loss1.backward()\n",
    "        model_optim.step()\n",
    "\n",
    "        train_loss += loss1.item()\n",
    "\n",
    "        _, output = torch.max(output, 1)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        label = label.cpu().detach().numpy()\n",
    "        for ii in range(len(label)):\n",
    "            if label[ii] != -1:\n",
    "                final_label.append(label[ii])\n",
    "                final_output.append(output[ii])\n",
    "        train_accuracy += f1_score(final_label, final_output, average='macro')\n",
    "    \n",
    "    train_loss = train_loss/train_dataloader_len\n",
    "    train_accuracy = train_accuracy/train_dataloader_len\n",
    "    history_train.append((train_loss, train_accuracy))\n",
    "    \n",
    "    model2.eval()   \n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    correct_val = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            final_label = []\n",
    "            final_output = []\n",
    "            inputs, label, inputs_len = data\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "            pred = model2(inputs, inputs_len)\n",
    "            pred = pred.to(device)\n",
    "            \n",
    "            pred = pred.view(-1, pred.size()[-1])\n",
    "            label = label.view(-1)\n",
    "            \n",
    "            loss2 = loss(pred, label)/batch_size\n",
    "            val_loss += loss2.item()\n",
    "            _, pred = torch.max(pred, 1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            label = label.cpu().detach().numpy()\n",
    "            for ii in range(len(label)):\n",
    "                if label[ii] != -1:\n",
    "                    final_label.append(label[ii])\n",
    "                    final_output.append(pred[ii])\n",
    "            val_accuracy += f1_score(final_label, final_output, average='macro')\n",
    "            \n",
    "        val_loss = val_loss/val_dataloader_len\n",
    "        val_accuracy = val_accuracy/val_dataloader_len\n",
    "        scheduler.step(val_accuracy)\n",
    "        history_val.append((val_loss, val_accuracy))\n",
    "        if val_accuracy > highest_val_score:\n",
    "            print(\"Saving model\")\n",
    "            highest_val_score = val_accuracy\n",
    "            torch.save({'model_state_dict': model2.state_dict()}, 'blstm2.pt')\n",
    "            \n",
    "    print(\"Epoch:\", (epoch+1))\n",
    "    print(\"Train Loss:\", train_loss, \"\\tTrain F1:\", train_accuracy)\n",
    "    print(\"Val Loss:\", val_loss, \"\\tVal F1:\", val_accuracy)\n",
    "    print(\"Current learning rate:\", model_optim.param_groups[0]['lr'])\n",
    "    print(\"=======================================================================================\")\n",
    "\n",
    "print(\"Final Val accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "389c3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []\n",
    "final_label = []\n",
    "model2.load_state_dict(torch.load('blstm2.pt'), strict=False)\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(val_dataloader, 0):\n",
    "\n",
    "        inputs, label, inputs_len = data\n",
    "        inputs, label = inputs.to(device), label.to(device)\n",
    "        pred = model2(inputs, inputs_len)\n",
    "        pred = pred.to(device)\n",
    "\n",
    "        pred = pred.view(-1, pred.size()[-1])\n",
    "        label = label.view(-1)\n",
    "        label = label.cpu().detach().numpy()\n",
    "\n",
    "        _, pred = torch.max(pred, 1)\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        \n",
    "        for ii in range(len(label)):\n",
    "            if label[ii] != -1:\n",
    "                final_output.append(pred[ii])\n",
    "                final_label.append(label[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f45869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = open('data/dev', 'r')\n",
    "lines = val.readlines()\n",
    "\n",
    "all_sentences = []\n",
    "cnt = 0\n",
    "for line in lines:\n",
    "    if line == '\\n':\n",
    "         all_sentences.append(line)\n",
    "    else:\n",
    "        cnt += 1\n",
    "        all_sentences.append(line.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46d35766",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "val_key = {val:key for key, val in tag_index.items()}\n",
    "with open(\"eval_dev2.out\", \"w\") as f:\n",
    "    for splits in all_sentences:\n",
    "            if splits == \"\\n\":\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "\n",
    "                idx, word, tag = splits\n",
    "                f.write(f\"{idx} {word} {tag.strip()} {val_key[final_output[i]]}\\n\")\n",
    "                i += 1\n",
    "i = 0         \n",
    "with open(\"dev2.out\", \"w\") as f:\n",
    "    for splits in all_sentences:\n",
    "            if splits == \"\\n\":\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "\n",
    "                idx, word, tag = splits\n",
    "                f.write(f\"{idx} {word} {val_key[final_output[i]]}\\n\")\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "deac5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []\n",
    "final_label = []\n",
    "model2.load_state_dict(torch.load('blstm2.pt'), strict=False)\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader, 0):\n",
    "\n",
    "        inputs, inputs_len = data\n",
    "        inputs = inputs.to(device)\n",
    "        pred = model2(inputs, inputs_len)\n",
    "        pred = pred.to(device)\n",
    "\n",
    "        pred = pred.view(-1, pred.size()[-1])\n",
    "\n",
    "        _, pred = torch.max(pred, 1)\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        \n",
    "        for ii in range(len(pred)):\n",
    "            if pred[ii] != -1:\n",
    "                final_output.append(pred[ii])\n",
    "\n",
    "test = open('data/test', 'r')\n",
    "lines = test.readlines()\n",
    "\n",
    "all_sentences = []\n",
    "cnt = 0\n",
    "for line in lines:\n",
    "    if line == '\\n':\n",
    "         all_sentences.append(line)\n",
    "    else:\n",
    "        cnt += 1\n",
    "        all_sentences.append(line.split(' '))\n",
    "        \n",
    "i = 0         \n",
    "test_key = {test:key for key, test in tag_index.items()}\n",
    "with open(\"test2.out\", \"w\") as f:\n",
    "    for splits in all_sentences:\n",
    "            if splits == \"\\n\":\n",
    "                f.write(\"\\n\")\n",
    "            else:\n",
    "\n",
    "                idx, word = splits\n",
    "                f.write(f\"{idx} {word.strip()} {test_key[final_output[i]]}\\n\")\n",
    "                i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052a6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
